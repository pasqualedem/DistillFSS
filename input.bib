@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})



@misc{Authors14,
 author = {FirstName LastName},
 title = {The frobnicatable foo filter},
 note = {Face and Gesture submission ID 324. Supplied as supplemental material {\tt fg324.pdf}},
 year = 2014
}

@misc{Authors14b,
 author = {FirstName LastName},
 title = {Frobnication tutorial},
 note = {Supplied as supplemental material {\tt tr.pdf}},
 year = 2014
}

@article{Alpher02,
author = {FirstName Alpher},
title = {Frobnication},
journal = PAMI,
volume = 12,
number = 1,
pages = {234--778},
year = 2002
}

@article{Alpher03,
author = {FirstName Alpher and  FirstName Fotheringham-Smythe},
title = {Frobnication revisited},
journal = {Journal of Foo},
volume = 13,
number = 1,
pages = {234--778},
year = 2003
}

@article{Alpher04,
author = {FirstName Alpher and FirstName Fotheringham-Smythe and FirstName Gamow},
title = {Can a machine frobnicate?},
journal = {Journal of Foo},
volume = 14,
number = 1,
pages = {234--778},
year = 2004
}

@inproceedings{Alpher05,
author = {FirstName Alpher and FirstName Gamow},
title = {Can a computer frobnicate?},
booktitle = ICCV,
pages = {234--778},
year = 2005
}


@article{chenDeepLabSemanticImage2018,
	title = {{DeepLab}: {Semantic} {Image} {Segmentation} with {Deep} {Convolutional} {Nets}, {Atrous} {Convolution}, and {Fully} {Connected} {CRFs}},
	volume = {40},
	issn = {1939-3539},
	shorttitle = {{DeepLab}},
	url = {https://ieeexplore.ieee.org/abstract/document/7913730},
	doi = {10.1109/TPAMI.2017.2699184},
	abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or `atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed “DeepLab” system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7 percent mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
	number = {4},
	urldate = {2025-03-03},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
	month = apr,
	year = {2018},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {atrous convolution, Computational modeling, conditional random fields, Context, Convolution, Convolutional neural networks, Image resolution, Image segmentation, Neural networks, semantic segmentation, Semantics},
	pages = {834--848},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\pasqu\\Zotero\\storage\\R2HMISN4\\7913730.html:text/html;Versione inviata:C\:\\Users\\pasqu\\Zotero\\storage\\4GDRXH6L\\Chen et al. - 2018 - DeepLab Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Con.pdf:application/pdf},
}


@article{everinghamPascalVisualObject2010,
	title = {The {Pascal} {Visual} {Object} {Classes} ({VOC}) {Challenge}},
	volume = {88},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-009-0275-4},
	doi = {10.1007/s11263-009-0275-4},
	abstract = {The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.},
	language = {en},
	number = {2},
	urldate = {2025-03-03},
	journal = {International Journal of Computer Vision},
	author = {Everingham, Mark and Van Gool, Luc and Williams, Christopher K. I. and Winn, John and Zisserman, Andrew},
	month = jun,
	year = {2010},
	keywords = {Artificial Intelligence, Benchmark, Database, Object detection, Object recognition},
	pages = {303--338},
	file = {Full Text PDF:C\:\\Users\\pasqu\\Zotero\\storage\\KFTF8ZMD\\Everingham et al. - 2010 - The Pascal Visual Object Classes (VOC) Challenge.pdf:application/pdf},
}


@inproceedings{heDeepResidualLearning2016a,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html},
	urldate = {2024-10-23},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2016},
	keywords = {notion},
	pages = {770--778},
	file = {Full Text PDF:C\:\\Users\\pasqu\\Zotero\\storage\\L93XD4JN\\He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:application/pdf},
}


@inproceedings{langLearningWhatNot2022,
	address = {New Orleans, LA, USA},
	title = {Learning {What} {Not} to {Segment}: {A} {New} {Perspective} on {Few}-{Shot} {Segmentation}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-6654-6946-3},
	shorttitle = {Learning {What} {Not} to {Segment}},
	url = {https://ieeexplore.ieee.org/document/9879241/},
	doi = {10.1109/CVPR52688.2022.00789},
	abstract = {Recently few-shot segmentation (FSS) has been extensively developed. Most previous works strive to achieve generalization through the meta-learning framework derived from classiﬁcation tasks; however, the trained models are biased towards the seen classes instead of being ideally class-agnostic, thus hindering the recognition of new concepts. This paper proposes a fresh and straightforward insight to alleviate the problem. Speciﬁcally, we apply an additional branch (base learner) to the conventional FSS model (meta learner) to explicitly identify the targets of base classes, i.e., the regions that do not need to be segmented. Then, the coarse results output by these two learners in parallel are adaptively integrated to yield precise segmentation prediction. Considering the sensitivity of meta learner, we further introduce an adjustment factor to estimate the scene differences between the input image pairs for facilitating the model ensemble forecasting. The substantial performance gains on PASCAL-5i and COCO-20i verify the effectiveness, and surprisingly, our versatile scheme sets a new state-of-the-art even with two plain learners. Moreover, in light of the unique nature of the proposed approach, we also extend it to a more realistic but challenging setting, i.e., generalized FSS, where the pixels of both base and novel classes are required to be determined. The source code is available at github.com/chunbolang/BAM.},
	language = {en},
	urldate = {2024-09-23},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Lang, Chunbo and Cheng, Gong and Tu, Binfei and Han, Junwei},
	month = jun,
	year = {2022},
	pages = {8047--8057},
	file = {Lang et al. - 2022 - Learning What Not to Segment A New Perspective on.pdf:C\:\\Users\\pasqu\\Zotero\\storage\\V5JVWPPZ\\Lang et al. - 2022 - Learning What Not to Segment A New Perspective on.pdf:application/pdf},
}


@inproceedings{kirillovSegmentAnything2023a,
	title = {Segment {Anything}},
	url = {https://openaccess.thecvf.com/content/ICCV2023/html/Kirillov_Segment_Anything_ICCV_2023_paper.html},
	language = {en},
	urldate = {2025-03-03},
	author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Dollar, Piotr and Girshick, Ross},
	year = {2023},
	pages = {4015--4026},
	file = {Full Text PDF:C\:\\Users\\pasqu\\Zotero\\storage\\8ICMFFX4\\Kirillov et al. - 2023 - Segment Anything.pdf:application/pdf},
}


@misc{huangRestNetBoostingCrossDomain2023,
	title = {{RestNet}: {Boosting} {Cross}-{Domain} {Few}-{Shot} {Segmentation} with {Residual} {Transformation} {Network}},
	shorttitle = {{RestNet}},
	url = {http://arxiv.org/abs/2308.13469},
	doi = {10.48550/arXiv.2308.13469},
	abstract = {Cross-domain few-shot segmentation (CD-FSS) aims to achieve semantic segmentation in previously unseen domains with a limited number of annotated samples. Although existing CD-FSS models focus on cross-domain feature transformation, relying exclusively on inter-domain knowledge transfer may lead to the loss of critical intra-domain information. To this end, we propose a novel residual transformation network (RestNet) that facilitates knowledge transfer while retaining the intra-domain support-query feature information. Specifically, we propose a Semantic Enhanced Anchor Transform (SEAT) module that maps features to a stable domain-agnostic space using advanced semantics. Additionally, an Intra-domain Residual Enhancement (IRE) module is designed to maintain the intra-domain representation of the original discriminant space in the new space. We also propose a mask prediction strategy based on prototype fusion to help the model gradually learn how to segment. Our RestNet can transfer cross-domain knowledge from both inter-domain and intra-domain without requiring additional fine-tuning. Extensive experiments on ISIC, Chest X-ray, and FSS-1000 show that our RestNet achieves state-of-the-art performance. Our code will be available soon.},
	urldate = {2025-03-03},
	publisher = {arXiv},
	author = {Huang, Xinyang and Zhu, Chuang and Chen, Wenkai},
	month = sep,
	year = {2023},
	note = {arXiv:2308.13469 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: BMVC 2023},
	file = {Preprint PDF:C\:\\Users\\pasqu\\Zotero\\storage\\33VCE2IZ\\Huang et al. - 2023 - RestNet Boosting Cross-Domain Few-Shot Segmentation with Residual Transformation Network.pdf:application/pdf;Snapshot:C\:\\Users\\pasqu\\Zotero\\storage\\ASITGVX6\\2308.html:text/html},
}


@incollection{leiCrossDomainFewShotSemantic2022,
	address = {Cham},
	title = {Cross-{Domain} {Few}-{Shot} {Semantic} {Segmentation}},
	volume = {13690},
	isbn = {978-3-031-20055-7 978-3-031-20056-4},
	url = {https://link.springer.com/10.1007/978-3-031-20056-4_5},
	language = {en},
	urldate = {2025-02-23},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Lei, Shuo and Zhang, Xuchao and He, Jianfeng and Chen, Fanglan and Du, Bowen and Lu, Chang-Tien},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	doi = {10.1007/978-3-031-20056-4_5},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {73--90},
	file = {PDF:C\:\\Users\\pasqu\\Zotero\\storage\\UH27I7ZV\\Lei et al. - 2022 - Cross-Domain Few-Shot Semantic Segmentation.pdf:application/pdf},
}


@inproceedings{liAdaptivePrototypeLearning2021,
	address = {Nashville, TN, USA},
	title = {Adaptive {Prototype} {Learning} and {Allocation} for {Few}-{Shot} {Segmentation}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-6654-4509-2},
	url = {https://ieeexplore.ieee.org/document/9578570/},
	doi = {10.1109/CVPR46437.2021.00823},
	language = {en},
	urldate = {2024-09-23},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Li, Gen and Jampani, Varun and Sevilla-Lara, Laura and Sun, Deqing and Kim, Jonghyun and Kim, Joongkyu},
	month = jun,
	year = {2021},
	pages = {8330--8339},
	file = {Li et al. - 2021 - Adaptive Prototype Learning and Allocation for Few.pdf:C\:\\Users\\pasqu\\Zotero\\storage\\ASXHJG4R\\Li et al. - 2021 - Adaptive Prototype Learning and Allocation for Few.pdf:application/pdf},
}


@inproceedings{longFullyConvolutionalNetworks2015,
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
	url = {https://openaccess.thecvf.com/content_cvpr_2015/html/Long_Fully_Convolutional_Networks_2015_CVPR_paper.html},
	urldate = {2025-03-03},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	year = {2015},
	pages = {3431--3440},
	file = {Full Text PDF:C\:\\Users\\pasqu\\Zotero\\storage\\I5HF9GPV\\Long et al. - 2015 - Fully Convolutional Networks for Semantic Segmentation.pdf:application/pdf},
}


@inproceedings{luSimplerBetterFewShot2021,
	title = {Simpler {Is} {Better}: {Few}-{Shot} {Semantic} {Segmentation} {With} {Classifier} {Weight} {Transformer}},
	shorttitle = {Simpler {Is} {Better}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Lu_Simpler_Is_Better_Few-Shot_Semantic_Segmentation_With_Classifier_Weight_Transformer_ICCV_2021_paper.html},
	language = {en},
	urldate = {2025-03-03},
	author = {Lu, Zhihe and He, Sen and Zhu, Xiatian and Zhang, Li and Song, Yi-Zhe and Xiang, Tao},
	year = {2021},
	pages = {8741--8750},
	file = {Full Text PDF:C\:\\Users\\pasqu\\Zotero\\storage\\QZF23C4X\\Lu et al. - 2021 - Simpler Is Better Few-Shot Semantic Segmentation With Classifier Weight Transformer.pdf:application/pdf},
}


@article{minHypercorrelationSqueezeFewShot,
	title = {Hypercorrelation {Squeeze} for {Few}-{Shot} {Segmentation}},
	language = {en},
	author = {Min, Juhong and Kang, Dahyun and Cho, Minsu},
	file = {Min et al. - Hypercorrelation Squeeze for Few-Shot Segmentation.pdf:C\:\\Users\\pasqu\\Zotero\\storage\\MD7DZWFT\\Min et al. - Hypercorrelation Squeeze for Few-Shot Segmentation.pdf:application/pdf},
}


@misc{oquabDINOv2LearningRobust2024,
	title = {{DINOv2}: {Learning} {Robust} {Visual} {Features} without {Supervision}},
	shorttitle = {{DINOv2}},
	url = {http://arxiv.org/abs/2304.07193},
	doi = {10.48550/arXiv.2304.07193},
	abstract = {The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.},
	urldate = {2025-03-03},
	publisher = {arXiv},
	author = {Oquab, Maxime and Darcet, Timothée and Moutakanni, Théo and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and Assran, Mahmoud and Ballas, Nicolas and Galuba, Wojciech and Howes, Russell and Huang, Po-Yao and Li, Shang-Wen and Misra, Ishan and Rabbat, Michael and Sharma, Vasu and Synnaeve, Gabriel and Xu, Hu and Jegou, Hervé and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
	month = feb,
	year = {2024},
	note = {arXiv:2304.07193 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\pasqu\\Zotero\\storage\\HNECDZNH\\Oquab et al. - 2024 - DINOv2 Learning Robust Visual Features without Supervision.pdf:application/pdf},
}


@inproceedings{pengHierarchicalDenseCorrelation2023,
	address = {Vancouver, BC, Canada},
	title = {Hierarchical {Dense} {Correlation} {Distillation} for {Few}-{Shot} {Segmentation}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-0129-8},
	url = {https://ieeexplore.ieee.org/document/10204860/},
	doi = {10.1109/CVPR52729.2023.02264},
	abstract = {Few-shot semantic segmentation (FSS) aims to form class-agnostic models segmenting unseen classes with only a handful of annotations. Previous methods limited to the semantic feature and prototype representation suffer from coarse segmentation granularity and train-set overfitting. In this work, we design Hierarchically Decoupled Matching Network (HDMNet) mining pixel-level support correlation based on the transformer architecture. The selfattention modules are used to assist in establishing hierarchical dense features, as a means to accomplish the cascade matching between query and support features. Moreover, we propose a matching module to reduce train-set overfitting and introduce correlation distillation leveraging semantic correspondence from coarse resolution to boost finegrained segmentation. Our method performs decently in experiments. We achieve 50.0\% mIoU on COCO-20i dataset one-shot setting and 56.0\% on five-shot segmentation, respectively. The code is available on the project website1.},
	language = {en},
	urldate = {2024-09-23},
	booktitle = {2023 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Peng, Bohao and Tian, Zhuotao and Wu, Xiaoyang and Wang, Chengyao and Liu, Shu and Su, Jingyong and Jia, Jiaya},
	month = jun,
	year = {2023},
	pages = {23641--23651},
	file = {Peng et al. - 2023 - Hierarchical Dense Correlation Distillation for Fe.pdf:C\:\\Users\\pasqu\\Zotero\\storage\\B29JXKCZ\\Peng et al. - 2023 - Hierarchical Dense Correlation Distillation for Fe.pdf:application/pdf},
}


@inproceedings{radfordLearningTransferableVisual2021,
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	url = {https://proceedings.mlr.press/v139/radford21a.html},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.},
	language = {en},
	urldate = {2025-03-03},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {8748--8763},
	file = {Full Text PDF:C\:\\Users\\pasqu\\Zotero\\storage\\F5L4W67H\\Radford et al. - 2021 - Learning Transferable Visual Models From Natural Language Supervision.pdf:application/pdf;Supplementary PDF:C\:\\Users\\pasqu\\Zotero\\storage\\QFDEJ9VN\\Radford et al. - 2021 - Learning Transferable Visual Models From Natural Language Supervision.pdf:application/pdf},
}


@misc{shabanOneShotLearningSemantic2017,
	title = {One-{Shot} {Learning} for {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/1709.03410},
	doi = {10.48550/arXiv.1709.03410},
	abstract = {Low-shot learning methods for image classification support learning from sparse data. We extend these techniques to support dense semantic image segmentation. Specifically, we train a network that, given a small set of annotated images, produces parameters for a Fully Convolutional Network (FCN). We use this FCN to perform dense pixel-level prediction on a test image for the new semantic class. Our architecture shows a 25\% relative meanIoU improvement compared to the best baseline methods for one-shot segmentation on unseen classes in the PASCAL VOC 2012 dataset and is at least 3 times faster.},
	urldate = {2025-03-03},
	publisher = {arXiv},
	author = {Shaban, Amirreza and Bansal, Shray and Liu, Zhen and Essa, Irfan and Boots, Byron},
	month = sep,
	year = {2017},
	note = {arXiv:1709.03410 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: To appear in the proceedings of the British Machine Vision Conference (BMVC) 2017. The code is available at https://github.com/lzzcd001/OSLSM},
	file = {Preprint PDF:C\:\\Users\\pasqu\\Zotero\\storage\\U48D686Z\\Shaban et al. - 2017 - One-Shot Learning for Semantic Segmentation.pdf:application/pdf;Snapshot:C\:\\Users\\pasqu\\Zotero\\storage\\KNDMUTWY\\1709.html:text/html},
}


@inproceedings{siamAMPAdaptiveMasked2019,
	title = {{AMP}: {Adaptive} {Masked} {Proxies} for {Few}-{Shot} {Segmentation}},
	shorttitle = {{AMP}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Siam_AMP_Adaptive_Masked_Proxies_for_Few-Shot_Segmentation_ICCV_2019_paper.html},
	urldate = {2025-03-03},
	author = {Siam, Mennatullah and Oreshkin, Boris N. and Jagersand, Martin},
	year = {2019},
	pages = {5249--5258},
	file = {Full Text PDF:C\:\\Users\\pasqu\\Zotero\\storage\\E34C95RR\\Siam et al. - 2019 - AMP Adaptive Masked Proxies for Few-Shot Segmentation.pdf:application/pdf},
}


@misc{simonyanVeryDeepConvolutional2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	doi = {10.48550/arXiv.1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	urldate = {2025-03-03},
	publisher = {arXiv},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = apr,
	year = {2015},
	note = {arXiv:1409.1556 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\pasqu\\Zotero\\storage\\LAVEP9E4\\Simonyan e Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:application/pdf;Snapshot:C\:\\Users\\pasqu\\Zotero\\storage\\PFK6MKS5\\1409.html:text/html},
}


@article{snellPrototypicalNetworksFewshot2017,
	title = {Prototypical networks for few-shot learning},
	volume = {30},
	journal = {Advances in neural information processing systems},
	author = {Snell, Jake and Swersky, Kevin and Zemel, Richard},
	year = {2017},
	keywords = {\#nosource, notion},
}


@inproceedings{taveraPixelbyPixelCrossDomainAlignment2022,
	title = {Pixel-by-{Pixel} {Cross}-{Domain} {Alignment} for {Few}-{Shot} {Semantic} {Segmentation}},
	url = {https://openaccess.thecvf.com/content/WACV2022/html/Tavera_Pixel-by-Pixel_Cross-Domain_Alignment_for_Few-Shot_Semantic_Segmentation_WACV_2022_paper.html},
	language = {en},
	urldate = {2025-03-03},
	author = {Tavera, Antonio and Cermelli, Fabio and Masone, Carlo and Caputo, Barbara},
	year = {2022},
	pages = {1626--1635},
}


@article{tianPriorGuidedFeature2022,
	title = {Prior {Guided} {Feature} {Enrichment} {Network} for {Few}-{Shot} {Segmentation}},
	volume = {44},
	issn = {0162-8828},
	url = {https://doi.org/10.1109/TPAMI.2020.3013717},
	doi = {10.1109/TPAMI.2020.3013717},
	abstract = {State-of-the-art semantic segmentation methods require sufficient labeled data to achieve good results and hardly work on unseen classes without fine-tuning. Few-shot segmentation is thus proposed to tackle this problem by learning a model that quickly adapts to new classes with a few labeled support samples. Theses frameworks still face the challenge of generalization ability reduction on unseen classes due to inappropriate use of high-level semantic information of training classes and spatial inconsistency between query and support targets. To alleviate these issues, we propose the Prior Guided Feature Enrichment Network (PFENet). It consists of novel designs of (1) a training-free prior mask generation method that not only retains generalization power but also improves model performance and (2) Feature Enrichment Module (FEM) that overcomes spatial inconsistency by adaptively enriching query features with support features and prior masks. Extensive experiments on PASCAL-5\&lt;inline-formula\&gt;\&lt;tex-math notation="LaTeX"\&gt;\${\textasciicircum}i\$\&lt;/tex-math\&gt;\&lt;alternatives\&gt;\&lt;mml:math\&gt;\&lt;mml:msup\&gt;\&lt;mml:mrow/\&gt;\&lt;mml:mi\&gt;i\&lt;/mml:mi\&gt;\&lt;/mml:msup\&gt;\&lt;/mml:math\&gt;\&lt;inline-graphic xlink:href="tian-ieq1-3013717.gif"/\&gt;\&lt;/alternatives\&gt;\&lt;/inline-formula\&gt; and COCO prove that the proposed prior generation method and FEM both improve the baseline method significantly. Our PFENet also outperforms state-of-the-art methods by a large margin without efficiency loss. It is surprising that our model even generalizes to cases without labeled support samples.},
	number = {2},
	urldate = {2024-09-18},
	journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
	author = {Tian, Zhuotao and Zhao, Hengshuang and Shu, Michelle and Yang, Zhicheng and Li, Ruiyu and Jia, Jiaya},
	month = feb,
	year = {2022},
	pages = {1050--1065},
	file = {Tian et al_2022_Prior Guided Feature Enrichment Network for Few-Shot Segmentation.pdf:C\:\\Users\\pasqu\\Desktop\\Pasquale\\Università\\Zotero\\Tian et al_2022_Prior Guided Feature Enrichment Network for Few-Shot Segmentation.pdf:application/pdf},
}


@inproceedings{wangPANetFewShotImage2019,
	address = {Seoul, Korea (South)},
	title = {{PANet}: {Few}-{Shot} {Image} {Semantic} {Segmentation} {With} {Prototype} {Alignment}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-7281-4803-8},
	shorttitle = {{PANet}},
	url = {https://ieeexplore.ieee.org/document/9010855/},
	doi = {10.1109/ICCV.2019.00929},
	abstract = {Despite the great progress made by deep CNNs in image semantic segmentation, they typically require a large number of densely-annotated images for training and are difﬁcult to generalize to unseen object categories. Few-shot segmentation has thus been developed to learn to perform segmentation from only a few annotated examples. In this paper, we tackle the challenging few-shot segmentation problem from a metric learning perspective and present PANet, a novel prototype alignment network to better utilize the information of the support set. Our PANet learns classspeciﬁc prototype representations from a few support images within an embedding space and then performs segmentation over the query images through matching each pixel to the learned prototypes. With non-parametric metric learning, PANet offers high-quality prototypes that are representative for each semantic class and meanwhile discriminative for different classes. Moreover, PANet introduces a prototype alignment regularization between support and query. With this, PANet fully exploits knowledge from the support and provides better generalization on few-shot segmentation. Signiﬁcantly, our model achieves the mIoU score of 48.1\% and 55.7\% on PASCAL-5i for 1-shot and 5-shot settings respectively, surpassing the state-of-the-art method by 1.8\% and 8.6\%.},
	language = {en},
	urldate = {2024-10-31},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Wang, Kaixin and Liew, Jun Hao and Zou, Yingtian and Zhou, Daquan and Feng, Jiashi},
	month = oct,
	year = {2019},
	pages = {9196--9205},
	file = {PDF:C\:\\Users\\pasqu\\Zotero\\storage\\3CWZD5HV\\Wang et al. - 2019 - PANet Few-Shot Image Semantic Segmentation With Prototype Alignment.pdf:application/pdf},
}


@inproceedings{wangRememberDifferenceCrossDomain2022,
	title = {Remember the {Difference}: {Cross}-{Domain} {Few}-{Shot} {Semantic} {Segmentation} via {Meta}-{Memory} {Transfer}},
	shorttitle = {Remember the {Difference}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Wang_Remember_the_Difference_Cross-Domain_Few-Shot_Semantic_Segmentation_via_Meta-Memory_Transfer_CVPR_2022_paper.html},
	language = {en},
	urldate = {2025-03-03},
	author = {Wang, Wenjian and Duan, Lijuan and Wang, Yuxi and En, Qing and Fan, Junsong and Zhang, Zhaoxiang},
	year = {2022},
	pages = {7065--7074},
	file = {Full Text PDF:C\:\\Users\\pasqu\\Zotero\\storage\\JPPEAI7J\\Wang et al. - 2022 - Remember the Difference Cross-Domain Few-Shot Semantic Segmentation via Meta-Memory Transfer.pdf:application/pdf},
}


@inproceedings{wangCrossDomainFewShotLearning2022,
	title = {Cross-{Domain} {Few}-{Shot} {Learning} for {Rare}-{Disease} {Skin} {Lesion} {Segmentation}},
	url = {https://ieeexplore.ieee.org/abstract/document/9746791},
	doi = {10.1109/ICASSP43922.2022.9746791},
	abstract = {Recently, deep learning (DL)-based skin lesion segmentation in dermoscopic images has advanced the efficient diagnosis of skin diseases. Commonly, most of the DL-based methods require a large amount of training data and can only perform accurate predictions on pre-defined classes. However, there exist some rare skin diseases with very limited labeled samples, which poses great challenges to typical DL-based methods. Few-shot learning (FSL) technique, which aims to train models with abundant seen classes and then generalizes to related unseen classes, is promising in addressing a similar problem. Unfortunately, simply borrowing the typical FSL is infeasible since collecting such abundant seen-class data (common skin diseases), is also difficult. In this paper, we propose a cross-domain few-shot segmentation (CD-FSS) framework, which enables the model to leverage the learning ability obtained from the natural domain, to facilitate rare-disease skin lesion segmentation with limited data of common diseases. Specifically, the framework consists of two processes, i.e., specific learning and generic learning, which are alternately optimized in a meta-training manner. A specific learner and a generic learner are tailored to build relationships between both processes. Experimental results demonstrate that our framework significantly improves the generalization ability from natural domain to unseen medical domain.},
	urldate = {2025-03-03},
	booktitle = {{ICASSP} 2022 - 2022 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Wang, Yixin and Xu, Zhe and Tian, Jiang and Luo, Jie and Shi, Zhongchao and Zhang, Yang and Fan, Jianping and He, Zhiqiang},
	month = may,
	year = {2022},
	note = {ISSN: 2379-190X},
	keywords = {Cross-domain, Data models, Deep learning, Few-shot segmentation, Image segmentation, Lesions, Signal processing, Skin, Skin lesion, Training data},
	pages = {1086--1090},
}


@inproceedings{wuLearningMetaClassMemory2021,
	title = {Learning {Meta}-{Class} {Memory} for {Few}-{Shot} {Semantic} {Segmentation}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Wu_Learning_Meta-Class_Memory_for_Few-Shot_Semantic_Segmentation_ICCV_2021_paper.html},
	language = {en},
	urldate = {2025-03-03},
	author = {Wu, Zhonghua and Shi, Xiangxi and Lin, Guosheng and Cai, Jianfei},
	year = {2021},
	pages = {517--526},
	file = {Full Text PDF:C\:\\Users\\pasqu\\Zotero\\storage\\7QVU7EUV\\Wu et al. - 2021 - Learning Meta-Class Memory for Few-Shot Semantic Segmentation.pdf:application/pdf},
}

@inproceedings{xieSegFormerSimpleEfficient2021,
	title = {{SegFormer}: {Simple} and {Efficient} {Design} for {Semantic} {Segmentation} with {Transformers}},
	volume = {34},
	shorttitle = {{SegFormer}},
	url = {https://proceedings.neurips.cc/paper/2021/hash/64f1f27bf1b4ec22924fd0acb550c235-Abstract.html},
	abstract = {We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perceptron (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efficient segmentation on Transformers.  We scale our approach up to obtain a series of models from SegFormer-B0 to Segformer-B5, which reaches much better performance and efficiency than previous counterparts.For example, SegFormer-B4 achieves 50.3\% mIoU on ADE20K with 64M parameters, being 5x smaller and 2.2\% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0\% mIoU on Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C.},
	urldate = {2023-05-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Xie, Enze and Wang, Wenhai and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M. and Luo, Ping},
	year = {2021},
	keywords = {\#nosource, notion},
	pages = {12077--12090},
	file = {Xie et al_2021_SegFormer.pdf:C\:\\Users\\pasqu\\Desktop\\Pasquale\\Università\\Zotero\\Xie et al_2021_SegFormer.pdf:application/pdf},
}


@inproceedings{yangMiningLatentClasses2021,
	title = {Mining {Latent} {Classes} for {Few}-{Shot} {Segmentation}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Yang_Mining_Latent_Classes_for_Few-Shot_Segmentation_ICCV_2021_paper.html},
	language = {en},
	urldate = {2025-03-03},
	author = {Yang, Lihe and Zhuo, Wei and Qi, Lei and Shi, Yinghuan and Gao, Yang},
	year = {2021},
	pages = {8721--8730},
}


@inproceedings{zhangSelfGuidedCrossGuidedLearning2021,
	address = {Nashville, TN, USA},
	title = {Self-{Guided} and {Cross}-{Guided} {Learning} for {Few}-{Shot} {Segmentation}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-6654-4509-2},
	url = {https://ieeexplore.ieee.org/document/9577637/},
	doi = {10.1109/CVPR46437.2021.00821},
	abstract = {Few-shot segmentation has been attracting a lot of attention due to its effectiveness to segment unseen object classes with a few annotated samples. Most existing approaches use masked Global Average Pooling (GAP) to encode an annotated support image to a feature vector to facilitate query image segmentation. However, this pipeline unavoidably loses some discriminative information due to the average operation. In this paper, we propose a simple but effective self-guided learning approach, where the lost critical information is mined. Speciﬁcally, through making an initial prediction for the annotated support image, the covered and uncovered foreground regions are encoded to the primary and auxiliary support vectors using masked GAP, respectively. By aggregating both primary and auxiliary support vectors, better segmentation performances are obtained on query images. Enlightened by our self-guided module for 1-shot segmentation, we propose a cross-guided module for multiple shot segmentation, where the ﬁnal mask is fused using predictions from multiple annotated samples with highquality support vectors contributing more and vice versa. This module improves the ﬁnal prediction in the inference stage without re-training. Extensive experiments show that our approach achieves new state-of-the-art performances on both PASCAL-5i and COCO-20i datasets. Source code is available at https://github.com/zbf1991/SCL .},
	language = {en},
	urldate = {2024-09-18},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zhang, Bingfeng and Xiao, Jimin and Qin, Terry},
	month = jun,
	year = {2021},
	pages = {8308--8317},
	file = {Zhang et al. - 2021 - Self-Guided and Cross-Guided Learning for Few-Shot.pdf:C\:\\Users\\pasqu\\Zotero\\storage\\6DGRGR9Q\\Zhang et al. - 2021 - Self-Guided and Cross-Guided Learning for Few-Shot.pdf:application/pdf},
}

@inproceedings{zhangPyramidGraphNetworks2019,
	title = {Pyramid {Graph} {Networks} {With} {Connection} {Attentions} for {Region}-{Based} {One}-{Shot} {Semantic} {Segmentation}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Pyramid_Graph_Networks_With_Connection_Attentions_for_Region-Based_One-Shot_Semantic_ICCV_2019_paper.html},
	urldate = {2025-03-03},
	author = {Zhang, Chi and Lin, Guosheng and Liu, Fayao and Guo, Jiushuang and Wu, Qingyao and Yao, Rui},
	year = {2019},
	pages = {9587--9595},
	file = {Full Text PDF:C\:\\Users\\pasqu\\Zotero\\storage\\LWSWGRE6\\Zhang et al. - 2019 - Pyramid Graph Networks With Connection Attentions for Region-Based One-Shot Semantic Segmentation.pdf:application/pdf},
}

@inproceedings{zhangFewShotSegmentationCycleConsistent2021,
	title = {Few-{Shot} {Segmentation} via {Cycle}-{Consistent} {Transformer}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/b8b12f949378552c21f28deff8ba8eb6-Abstract.html},
	abstract = {Few-shot segmentation aims to train a segmentation model that can fast adapt to novel classes with few exemplars. The conventional training paradigm is to learn to make predictions on query images conditioned on the features from support images. Previous methods only utilized the semantic-level prototypes of support images as the conditional information. These methods cannot utilize all pixel-wise support information for the query predictions, which is however critical for the segmentation task. In this paper, we focus on utilizing pixel-wise relationships between support and target images to facilitate the few-shot semantic segmentation task. We design a novel Cycle-Consistent Transformer (CyCTR) module to aggregate pixel-wise support features into query ones. CyCTR performs cross-attention between features from different images, i.e. support and query images. We observe that there may exist unexpected irrelevant pixel-level support features. Directly performing cross-attention may aggregate these features from support to query and bias the query features. Thus, we propose using a novel cycle-consistent attention mechanism to filter out possible harmful support features and encourage query features to attend to the most informative pixels from support images. Experiments on all few-shot segmentation benchmarks demonstrate that our proposed CyCTR leads to remarkable improvement compared to previous state-of-the-art methods. Specifically, on Pascal-5{\textasciicircum}i and COCO-20{\textasciicircum}i datasets, we achieve 66.6\% and 45.6\% mIoU for 5-shot segmentation, outperforming previous state-of-the-art by 4.6\% and 7.1\% respectively.},
	urldate = {2025-03-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhang, Gengwei and Kang, Guoliang and Yang, Yi and Wei, Yunchao},
	year = {2021},
	pages = {21984--21996},
	file = {Full Text PDF:C\:\\Users\\pasqu\\Zotero\\storage\\8DR2PMN7\\Zhang et al. - 2021 - Few-Shot Segmentation via Cycle-Consistent Transformer.pdf:application/pdf},
}


@inproceedings{linCANetClassAgnosticSegmentation2019,
	title = {{CANet}: {Class}-{Agnostic} {Segmentation} {Networks} {With} {Iterative} {Refinement} and {Attentive} {Few}-{Shot} {Learning}},
	shorttitle = {{CANet}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_CANet_Class-Agnostic_Segmentation_Networks_With_Iterative_Refinement_and_Attentive_Few-Shot_CVPR_2019_paper.html},
	urldate = {2025-03-03},
	author = {Lin, Guosheng and Liu, Fayao and Yao, Rui and Shen, Chunhua},
	year = {2019},
	pages = {5217--5226},
}


@incollection{liuPartAwarePrototypeNetwork2020,
	address = {Cham},
	title = {Part-{Aware} {Prototype} {Network} for {Few}-{Shot} {Semantic} {Segmentation}},
	volume = {12354},
	isbn = {978-3-030-58544-0 978-3-030-58545-7},
	url = {https://link.springer.com/10.1007/978-3-030-58545-7_9},
	language = {en},
	urldate = {2024-11-01},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Liu, Yongfei and Zhang, Xiangyi and Zhang, Songyang and He, Xuming},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58545-7_9},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {142--158},
	file = {PDF:C\:\\Users\\pasqu\\Zotero\\storage\\GU8MMP37\\Liu et al. - 2020 - Part-Aware Prototype Network for Few-Shot Semantic Segmentation.pdf:application/pdf},
}

@inproceedings{yangPrototypeMixtureModels2020,
	address = {Cham},
	title = {Prototype {Mixture} {Models} for {Few}-{Shot} {Semantic} {Segmentation}},
	isbn = {978-3-030-58598-3},
	doi = {10.1007/978-3-030-58598-3_45},
	abstract = {Few-shot segmentation is challenging because objects within the support and query images could significantly differ in appearance and pose. Using a single prototype acquired directly from the support image to segment the query image causes semantic ambiguity. In this paper, we propose prototype mixture models (PMMs), which correlate diverse image regions with multiple prototypes to enforce the prototype-based semantic representation. Estimated by an Expectation-Maximization algorithm, PMMs incorporate rich channel-wised and spatial semantics from limited support images. Utilized as representations as well as classifiers, PMMs fully leverage the semantics to activate objects in the query image while depressing background regions in a duplex manner. Extensive experiments on Pascal VOC and MS-COCO datasets show that PMMs significantly improve upon state-of-the-arts. Particularly, PMMs improve 5-shot segmentation performance on MS-COCO by up to 5.82\% with only a moderate cost for model size and inference speed (Code is available at github.com/Yang-Bob/PMMs.).},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Yang, Boyu and Liu, Chang and Li, Bohao and Jiao, Jianbin and Ye, Qixiang},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {763--778},
}

@inproceedings{hoffmanCyCADACycleConsistentAdversarial2018,
	title = {{CyCADA}: {Cycle}-{Consistent} {Adversarial} {Domain} {Adaptation}},
	shorttitle = {{CyCADA}},
	url = {https://proceedings.mlr.press/v80/hoffman18a.html},
	abstract = {Domain adaptation is critical for success in new, unseen environments. Adversarial adaptation models have shown tremendous progress towards adapting to new environments by focusing either on discovering domain invariant representations or by mapping between unpaired image domains. While feature space methods are difficult to interpret and sometimes fail to capture pixel-level and low-level domain shifts, image space methods sometimes fail to incorporate high level semantic knowledge relevant for the end task. We propose a model which adapts between domains using both generative image space alignment and latent representation space alignment. Our approach, Cycle-Consistent Adversarial Domain Adaptation (CyCADA), guides transfer between domains according to a specific discriminatively trained task and avoids divergence by enforcing consistency of the relevant semantics before and after adaptation. We evaluate our method on a variety of visual recognition and prediction settings, including digit classification and semantic segmentation of road scenes, advancing state-of-the-art performance for unsupervised adaptation from synthetic to real world driving domains.},
	language = {en},
	urldate = {2025-03-03},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Hoffman, Judy and Tzeng, Eric and Park, Taesung and Zhu, Jun-Yan and Isola, Phillip and Saenko, Kate and Efros, Alexei and Darrell, Trevor},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {1989--1998},
	file = {Full Text PDF:C\:\\Users\\pasqu\\Zotero\\storage\\3EYWBX9G\\Hoffman et al. - 2018 - CyCADA Cycle-Consistent Adversarial Domain Adaptation.pdf:application/pdf},
}

@inproceedings{longConditionalAdversarialDomain2018,
	title = {Conditional {Adversarial} {Domain} {Adaptation}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/hash/ab88b15733f543179858600245108dd8-Abstract.html},
	abstract = {Adversarial learning has been embedded into deep networks to learn disentangled and transferable representations for domain adaptation. Existing adversarial domain adaptation methods may struggle to align different domains of multimodal distributions that are native in classification problems. In this paper, we present conditional adversarial domain adaptation, a principled framework that conditions the adversarial adaptation models on discriminative information conveyed in the classifier predictions. Conditional domain adversarial networks (CDANs) are designed with two novel conditioning strategies: multilinear conditioning that captures the cross-covariance between feature representations and classifier predictions to improve the discriminability, and entropy conditioning that controls the uncertainty of classifier predictions to guarantee the transferability. Experiments testify that the proposed approach exceeds the state-of-the-art results on five benchmark datasets.},
	urldate = {2025-03-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Long, Mingsheng and CAO, ZHANGJIE and Wang, Jianmin and Jordan, Michael I},
	year = {2018},
}


@inproceedings{zouUnsupervisedDomainAdaptation2018,
	title = {Unsupervised {Domain} {Adaptation} for {Semantic} {Segmentation} via {Class}-{Balanced} {Self}-{Training}},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Yang_Zou_Unsupervised_Domain_Adaptation_ECCV_2018_paper.html},
	urldate = {2025-03-03},
	author = {Zou, Yang and Yu, Zhiding and Kumar, B. V. K. Vijaya and Wang, Jinsong},
	year = {2018},
	pages = {289--305},
	file = {Full Text PDF:C\:\\Users\\pasqu\\Zotero\\storage\\YVLFFJZF\\Zou et al. - 2018 - Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training.pdf:application/pdf},
}

@inproceedings{panSwitchableWhiteningDeep2019,
	title = {Switchable {Whitening} for {Deep} {Representation} {Learning}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Pan_Switchable_Whitening_for_Deep_Representation_Learning_ICCV_2019_paper.html},
	urldate = {2025-03-03},
	author = {Pan, Xingang and Zhan, Xiaohang and Shi, Jianping and Tang, Xiaoou and Luo, Ping},
	year = {2019},
	pages = {1863--1871},
	file = {Full Text PDF:C\:\\Users\\pasqu\\Zotero\\storage\\25QEP9LW\\Pan et al. - 2019 - Switchable Whitening for Deep Representation Learning.pdf:application/pdf},
}

@article{pengGlobalLocalTexture2021,
	title = {Global and {Local} {Texture} {Randomization} for {Synthetic}-to-{Real} {Semantic} {Segmentation}},
	volume = {30},
	issn = {1941-0042},
	url = {https://ieeexplore.ieee.org/abstract/document/9489280},
	doi = {10.1109/TIP.2021.3096334},
	abstract = {Semantic segmentation is a crucial image understanding task, where each pixel of image is categorized into a corresponding label. Since the pixel-wise labeling for ground-truth is tedious and labor intensive, in practical applications, many works exploit the synthetic images to train the model for real-word image semantic segmentation, i.e., Synthetic-to-Real Semantic Segmentation (SRSS). However, Deep Convolutional Neural Networks (CNNs) trained on the source synthetic data may not generalize well to the target real-world data. To address this problem, there has been rapidly growing interest in Domain Adaption technique to mitigate the domain mismatch between the synthetic and real-world images. Besides, Domain Generalization technique is another solution to handle SRSS. In contrast to Domain Adaption, Domain Generalization seeks to address SRSS without accessing any data of the target domain during training. In this work, we propose two simple yet effective texture randomization mechanisms, Global Texture Randomization (GTR) and Local Texture Randomization (LTR), for Domain Generalization based SRSS. GTR is proposed to randomize the texture of source images into diverse unreal texture styles. It aims to alleviate the reliance of the network on texture while promoting the learning of the domain-invariant cues. In addition, we find the texture difference is not always occurred in entire image and may only appear in some local areas. Therefore, we further propose a LTR mechanism to generate diverse local regions for partially stylizing the source images. Finally, we implement a regularization of Consistency between GTR and LTR (CGL) aiming to harmonize the two proposed mechanisms during training. Extensive experiments on five publicly available datasets (i.e., GTA5, SYNTHIA, Cityscapes, BDDS and Mapillary) with various SRSS settings (i.e., GTA5/SYNTHIA to Cityscapes/BDDS/Mapillary) demonstrate that the proposed method is superior to the state-of-the-art methods for domain generalization based SRSS.},
	urldate = {2025-03-03},
	journal = {IEEE Transactions on Image Processing},
	author = {Peng, Duo and Lei, Yinjie and Liu, Lingqiao and Zhang, Pingping and Liu, Jun},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Adaptation models, Complexity theory, consistency regularization, domain generalization, Image segmentation, Painting, Semantics, Synthetic-to-real semantic segmentation, Task analysis, texture randomization, Training},
	pages = {6594--6608},
}

@inproceedings{pengSemanticAwareDomainGeneralized2022,
	title = {Semantic-{Aware} {Domain} {Generalized} {Segmentation}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Peng_Semantic-Aware_Domain_Generalized_Segmentation_CVPR_2022_paper.html},
	language = {en},
	urldate = {2025-03-03},
	author = {Peng, Duo and Lei, Yinjie and Hayat, Munawar and Guo, Yulan and Li, Wen},
	year = {2022},
	pages = {2594--2605},
	file = {Full Text PDF:C\:\\Users\\pasqu\\Zotero\\storage\\TQFDH4ZA\\Peng et al. - 2022 - Semantic-Aware Domain Generalized Segmentation.pdf:application/pdf},
}

@inproceedings{huangFSDRFrequencySpace2021,
	title = {{FSDR}: {Frequency} {Space} {Domain} {Randomization} for {Domain} {Generalization}},
	shorttitle = {{FSDR}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Huang_FSDR_Frequency_Space_Domain_Randomization_for_Domain_Generalization_CVPR_2021_paper.html},
	language = {en},
	urldate = {2025-03-03},
	author = {Huang, Jiaxing and Guan, Dayan and Xiao, Aoran and Lu, Shijian},
	year = {2021},
	pages = {6891--6902},
	file = {Full Text PDF:C\:\\Users\\pasqu\\Zotero\\storage\\ZSMVYRMR\\Huang et al. - 2021 - FSDR Frequency Space Domain Randomization for Domain Generalization.pdf:application/pdf},
}

@misc{luCrossdomainFewshotSegmentation2022,
	title = {Cross-domain {Few}-shot {Segmentation} with {Transductive} {Fine}-tuning},
	url = {http://arxiv.org/abs/2211.14745},
	doi = {10.48550/arXiv.2211.14745},
	abstract = {Few-shot segmentation (FSS) expects models trained on base classes to work on novel classes with the help of a few support images. However, when there exists a domain gap between the base and novel classes, the state-of-the-art FSS methods may even fail to segment simple objects. To improve their performance on unseen domains, we propose to transductively fine-tune the base model on a set of query images under the few-shot setting, where the core idea is to implicitly guide the segmentation of query images using support labels. Although different images are not directly comparable, their class-wise prototypes are desired to be aligned in the feature space. By aligning query and support prototypes with an uncertainty-aware contrastive loss, and using a supervised cross-entropy loss and an unsupervised boundary loss as regularizations, our method could generalize the base model to the target domain without additional labels. We conduct extensive experiments under various cross-domain settings of natural, remote sensing, and medical images. The results show that our method could consistently and significantly improve the performance of prototypical FSS models in all cross-domain tasks.},
	urldate = {2025-03-03},
	publisher = {arXiv},
	author = {Lu, Yuhang and Wu, Xinyi and Wu, Zhenyao and Wang, Song},
	month = nov,
	year = {2022},
	note = {arXiv:2211.14745 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 12 pages, 8 figures},
	file = {Preprint PDF:C\:\\Users\\pasqu\\Zotero\\storage\\ENMMFHK9\\Lu et al. - 2022 - Cross-domain Few-shot Segmentation with Transductive Fine-tuning.pdf:application/pdf;Snapshot:C\:\\Users\\pasqu\\Zotero\\storage\\3RB348WW\\2211.html:text/html},
}
